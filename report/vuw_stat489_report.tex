\documentclass{article}

\usepackage{graphicx, amsmath}
\usepackage{subcaption}

% 0. Referencing style, APA-like referencing
\usepackage[backend=biber, style=apa, natbib=true]{biblatex}
\usepackage{algorithm}
\usepackage{algorithmic}
\addbibresource{bibliography.bib}

% 1.  Don't need such wide margins.
\usepackage[margin=1in]{geometry} % Setting the margins to 1 inch

% \usepackage{amsmath}

\begin{document}

\title{Ordinal Data clustering and prediction}

\author{Quan Zhao}

\maketitle

% From VUW 487 Report
\section{Introduction}


The study of ordinal data presents unique challenges and opportunities.
Ordinal data consists of categories with an inherent order but undefined
spacing. This makes it distinct from nominal and continuous data,
necessitating specialized analytical methods that acknowledge its
structured ordering without presuming equal intervals between its
categories.

This project investigates methods for clustering and predicting cluster
membership for ordinal data. These methods are useful because ordinal
data is prevalent in fields ranging from social sciences to healthcare.
We begin by defining ordinal data and describing its distinctive
features. We then discuss existing clustering and prediction
methodologies. We focus on model-based clustering because we have an
existing sophisticated approach of this type, that is tailored to the
ordinal nature of the data, whereas there are limitations inherent in
distance-based methods when applied to ordinal data. The report
describes specific statistical models developed for ordinal data
analysis, such as the Proportional Odds model and the Ordered Stereotype
Model, highlighting their effectiveness in extracting meaningful
insights.

We then reflect on the historical development of ordinal data analysis,
from early methodologies to recent innovations that have broadened its
applicability and improved its precision. We also consider future
research directions, including the potential for integrating machine
learning with conventional statistical methods to refine the predictive
performance of ordinal data models.

Clustering methods have previously been developed for ordinal data,
using models such as the Ordered Stereotype Model. However, these
existing methods do not include techniques for determining the likely
cluster membership of any new observations that were not available at
the initial clustering stage. Therefore, this research aims to extend
these existing methods to accurately predict cluster membership for
incoming data points. Such advancements are crucial for applying ordinal
data in dynamic environments where new data are produced frequently. The
accuracy of the predictions is also important, so we will assess this to
ensure that the methods perform sufficiently well.

In summary, this paper offers an examination of methodologies for
clustering and predicting ordinal data. This inquiry is intended as a
resource for both researchers and practitioners, providing insights
necessary for navigating the complexities of ordinal data and leveraging
its full potential in analytical endeavors.

\subsection{Ordinal Data}

Ordinal data, a pivotal concept in statistical analysis, represents categorical data characterized by a meaningful order among its categories, without implying uniform differences between these ranks. Unlike nominal data, which merely categorizes without any implied ranking, ordinal data elevates the categorical analysis by introducing a hierarchy or sequence that is significant yet lacks quantifiable intervals between its elements. This characteristic distinguishes ordinal data: it conveys the sequence of values but remains silent on the magnitude of difference between successive ranks.

Such data often appear as rankings or ordered classifications where the precise distance between categories is undefined or irrelevant. 
Despite sometimes being numerically coded for analytical convenience, ordinal data resist traditional arithmetic operations, rendering calculations like addition or subtraction inappropriate and misleading.

In the realm of statistical analysis, ordinal data necessitates specialized, non-parametric methods. 
The median and mode stand out as appropriate measures of central tendency for this data types, 
aligning with their ordered nature without assuming equal intervals between categories.

Ordinal data's prevalence is notably high in social sciences, especially in surveys and questionnaires designed to capture subjective assessments such as attitudes, opinions, and preferences. These instruments frequently employ scales—ranging from ``strongly disagree'' to ``strongly agree''—to elicit responses that, while ordered, do not support the notion of fixed distances between adjacent categories. This principle underlines the core attribute of ordinal data: the clear hierarchy among responses without an inherent interval scale to quantify the gaps between them. Consequently, assigning numerical values to such categories for computational purposes is generally discouraged and conceptually flawed, as it misinterprets the data's ordinal nature 

(~\textcite{Johnson1999}) gives a good sample,
 It is typically rational to presume an order such as

 \[
 \text{strongly disagree} < \text{disagree} < \text{don’t know} < \text{agree} < \text{strongly agree}
\]
 However, it is often illogical to allocate integer values to these categories. Therefore, calculations like
\[
\text{``disagree''} - \text{``strongly disagree''}
\]

\[
\text{``agree''} - \text{``don't know''}
\]
are not considered valid. 
This demonstrates that performing arithmetic calculations on the numerical values assigned to the categories does not make sense.

% \begin{figure}[ht!] % 'h!' places the figure here, in the text
%     \centering % Centers the figure
%     \includegraphics[width=0.6\textwidth]{images/ordinal_data_dist.png} % Include the image with 50% of the text width
%     \caption{Latent trait interpretation of ordinal classification. 
%     In this plot, the logistic density represents the distribution of latent traits for a particular individual. 
%     It is assumed that a random variable is drawn from this density, and the value of this random variable determines an individual's classification. 
%     For example, if a deviate of 0.5 is drawn, the individual receives a D. (~\cite{Johnson1999}).} % Caption for the image
%     \label{fig:ordinal} % Label for referencing the figure in the text
%   \end{figure}

\subsection{Comparison with continuous numerical data}

In the context of statistical analysis, the distinction between ordinal and continuous numerical data types is pivotal, influencing both the choice of analytical methods and the depth of insights that can be derived. 
Ordinal data, characterized by its capacity to rank order categories without indicating precise differences between them, is inherently less informative than continuous numerical data. 
Continuous numerical data, with its quantitative nature, allows for an infinite range of values and supports detailed statistical operations, including arithmetic calculations and the application of advanced statistical models, facilitating a nuanced understanding of variables and their interrelations (~\cite{Stevens1946}).

The limitations of ordinal data stem from its inability to quantify the exact magnitude of differences between categories, a factor that restricts the application of parametric statistical methods. This constraint necessitates reliance on non-parametric methods, focusing on medians and modes rather than means and standard deviations, thereby offering a less detailed analysis (~\cite{Conover1999}). 
For instance, in Likert scale responses commonly used in surveys, the ordinal nature of data precludes meaningful calculations of averages or differences between responses, limiting the depth of analysis that can be achieved (~\cite{Likert1932}).

Comparatively, continuous numerical data's capacity for precise measurement and the application of a broader range of statistical analyses enables a more detailed and accurate exploration of phenomena. 
This capability is contingent upon the availability of continuous numerical results, which may not always be feasible. The nature of the data collectible in certain research scenarios—such as measuring opinions, attitudes, or perceptions—often necessitates the use of ordinal data, due to the challenges associated with obtaining continuous measurements in these contexts. For instance, eliciting responses on a continuous percentage scale from 0 to 100 can be impractical when assessing subjective experiences or preferences. Hence, the decision to utilize ordinal data over continuous numerical data is not solely a matter of choice in research design and analysis but is also influenced by the practicalities of what data can realistically be gathered. This consideration is crucial for ensuring the validity and relevance of research findings, highlighting the importance of aligning data collection methods with the specific circumstances and constraints of the study.

In summary, while ordinal data is invaluable for capturing rankings and subjective assessments, 
its analytical limitations highlight the superior informational value of continuous numerical data in quantitative research. 
This distinction is crucial for researchers in the selection of appropriate statistical methods and in the interpretation of their data, 
ensuring that conclusions drawn are both valid and meaningful.

\subsection{Clustering}

Clustering is a fundamental technique in the field of data analysis and machine learning, aimed at grouping a set of objects in such a way that objects in the same group (or cluster) are more similar to each other than to those in other groups. Its applications span a wide range of areas including market research, pattern recognition, image analysis, and bioinformatics, among others. For instance, in market research, clustering can help identify distinct customer segments based on purchasing behavior, while in bioinformatics, it can be used to classify different types of genes or proteins with similar functions.

There are primarily two types of clustering: distance-based clustering and model-based clustering.

1. \textbf{Distance-based Clustering}: 

This approach relies on the concept of similarity or distance between data points. 
The aim is to minimize the distance between data points within a cluster while maximizing the distance between data points in different clusters. 
Popular methods include K-means (~\cite{macqueen1967some}), 
hierarchical clustering (~\cite{johnson1967hierarchical}), and DBSCAN (~\cite{ester1996density}), 
each using different metrics (e.g., Euclidean, Manhattan) to measure distance or similarity.

2. \textbf{Model-based Clustering}: 

Unlike distance-based methods, model-based clustering (~\cite{fraley2002model}) assumes that the data is generated from a mixture of finite distributions, 
where each component of the mixture represents a cluster. 
This approach tries to estimate the parameters of these distributions to optimize the fit between the model and the data. 
Model-based methods are particularly useful for complex data structures, including those with mixed types of data (numerical, categorical) or where the clusters have different sizes, shapes, or orientations.

Model-based clustering is often more suitable for ordinal data—data that contain natural, orderable categories. This is because ordinal data, which embody an inherent ranking or order (e.g., survey responses from "strongly agree" to "strongly disagree"), do not fit well with the distance metrics used in distance-based clustering, which treat distances between all pairs of categories as equal. Model-based clustering, on the other hand, can accommodate the ordinal nature by incorporating it into the model assumptions, allowing for a more nuanced understanding and grouping based on the inherent order of the data. This method can capture the ordinal information effectively, providing more meaningful and interpretable clusters for analysis.

\section{History of model-based Ordinal Data Clustering}

\subsection{Early Developments (2000--2010)}

McLachlan \& Basford 1988 (~\cite{mclachlan1988mixture}) start to marked a crucial step in mixture model applications by simplifying the maximum likelihood estimation (MLE) using the EM algorithm (~\cite{dempster1977maximum}).

McLachlan \& Peel's 2000 book further elaborate on this approach. This approach, utilizing $Y_j$ and $Z_j$, not only enhanced the computational efficiency of MLE but also laid a foundational strategy for Bayesian approaches and MCMC methods in mixture models. The paper’s impact is evident in its widespread adoption across various domains, from bioinformatics to finance, where mixture models are employed (~\cite{mclachlan2000finite}).

In 2002, Figueiredo's introduction of an unsupervised algorithm for learning finite mixture models was a game-changer. This method's ability to autonomously select the number of components represented a significant leap over previous techniques, which often relied on arbitrary or manual component selection. Additionally, the algorithm's robustness against initialization issues and singular estimates made it a go-to choice for practitioners dealing with complex multivariate data (~\cite{figueiredo2002unsupervised}).

A key paper in 2010 by Volodymyr and Ranjan addressed practical challenges in applying the EM algorithm for mixture models. This comprehensive guide to estimation, model selection, and likelihood maximization was a boon for both researchers and practitioners. Notably, the work extended beyond Gaussian mixtures, offering insights and methodologies for simulating and visualizing non-Gaussian mixtures, thereby broadening the applicability of mixture models (~\cite{10.1214/09-SS053}).

\subsection{Recent Developments (2010--2019)}

In 2016, Matechou et al. introduced a novel approach to data analysis with their proposal of finite mixture models for biclustering two-mode ordinal categorical data. Utilizing proportional odds parameterization, these models offered a sophisticated understanding of complex data patterns, particularly beneficial in fields such as genomics and social sciences where ordinal data is prevalent. The application of the EM algorithm for model fitting highlighted the continued importance of this method in the context of mixture model applications (~\cite{matechou2016biclustering}).

Similarly, in 2016, Fernandez et al. presented an alternative methodology for clustering ordinal data by employing likelihood-based methods through finite mixtures with the stereotype model. This approach stood out for its use in fuzzy clustering techniques, an area that has been attracting increased attention in the realm of data science (~\cite{fernandez2016mixture}).

In 2019 Fernandez et al.'s extension of finite mixture models to binary, count, and ordinal data under a unified statistical framework represented a consolidation and expansion of mixture model applications. The introduction of maximum likelihood estimation parameters and the Bayesian approach for simultaneous estimation were indicative of the field's progression towards more flexible and comprehensive modeling techniques (~\cite{fernandez2019finite}).

The primary innovation in both of these studies lies in the integration of ordinal data models with finite mixture methods, a departure from the traditional use of finite mixtures for continuous variables.


Jacques and Biernacki's 2018 introduction of a model-based co-clustering algorithm was a significant advancement. The algorithm's ability to handle missing data and its interpretability made it especially relevant for high-dimensional datasets. The BOS distribution employed in this model underscored the continuous innovation in probabilistic modeling techniques, catering to the increasing complexity of data in modern research (~\cite{jacques2018model}).

Compare with previous work, Jacques and Biernacki's (~\cite{jacques2018model}) method is based on a novel Binary Ordinal Search (BOS) model emphasizing efficiency and interpretability, especially with missing data. 

\subsection{Compare with other clustering algorithm}

Compared to tree based or distance based clustering algorithms, model-based clustering algorithms demonstrate superior performance with ordinal data, particularly in scenarios involving multiclass and multioutput cases. 
This advantage stems from two key factors: firstly, ordinal data do not presuppose equal distances between categories, a condition that distance-based methods often rely on. Secondly, ordinal data typically encompass only a few categories, which can limit the effectiveness of tree-based methods designed to partition data across a broader numerical range.

% End From VUW 487 Report

\section{Addition work (TODO)}

TODO

\section{Statistical-based Ordinal Data Clustering}

\subsection{Ordered Stereotype Model}

The Ordered Stereotype Model  (~\cite{anderson1984regression}) is a statistical approach designed to analyze ordinal dependent variables, 
where the outcomes are categories with a natural order but not a quantifiable difference between them.

In this work, we follow the methodology outlined by (~\cite{fernandez2016mixture})
Given an ordinal response variable $Y_{ij}$ with categories ($k=1, 2, \ldots, q$), and $i$ is row index, $j$ is column index.
The probability of $Y_{ij}$ falling into the $k$th category, is denoted as $P(Y = k)$.
The $g$ is cluster of data, if we have $g$ clusters.

The model for category $k$ is then:
\begin{equation}
  \log\left(\frac{P(Y = k)}{P(Y = 1)} \mid i \in g, j \in J\right) = \mu_k + \phi_k \times \left(\alpha_g + \beta_j + X_i^T \cdot \theta \right)
\end{equation}
  

where $\mu_k$ is the intercept for category $k$, $\mu_1$ set to zero for identical probability. 
 $\alpha$ represents the effect of cluster $g$,
and $\beta$ represents the effect of column $J$.
Parameters $\phi$ must be arranged in an ordered sequence from 
$0 = \phi_1 \leq \phi_2 \leq \ldots \leq \phi_K = 1.$ 
This constraint allows the model to adapt to the inherent ordering of categories, ensuring the effects across categories follow a common, scaled pattern.


  

% 123

\subsection{Expectation-Maximization (EM) Algorithm}

% EM algorithm has been introduced by Dempster, Laird and Rubin in 1977. (~\cite*[Dempster, Laird and Rubin]{Dempster1977}).

% The Expectation-Maximization (EM) algorithm is a two-step iterative method to obtain the maximum likelihood estimate (MLE) of the parameters of a statistical model, where the model depends on unobserved latent variables. The EM algorithm is particularly useful for mixture models, where the data is considered to be generated from a combination of several different statistical distributions, each representing a different 'component' of the population.

% \subsubsection*{Expectation-Maximization Algorithm for Mixture Model}

% A mixture model is a probabilistic model for representing the presence of subpopulations within an overall population, without requiring that an observed data set explicitly identify the subpopulation to which an individual observation belongs. Formally, if we assume a mixture of $K$ components, the probability density function (pdf) of a mixture model can be written as:

% \begin{equation}
% p(x|\Theta) = \sum_{k=1}^{K} \pi_k f_k(x|\theta_k)
% \end{equation}

% where $x$ represents the data points, $\Theta$ represents the parameters of the mixture model which include both the mixing coefficients $\pi_k$ and the parameters of the component distributions $\theta_k$, $f_k$ is the component distribution, and $\pi_k$ are the mixing coefficients such that $\sum_{k=1}^{K} \pi_k = 1$ and $\pi_k \ge 0$.

% \subsubsection{Introduction to the EM Algorithm}
The Expectation-Maximization (EM) algorithm is a powerful statistical tool for finding maximum likelihood estimates in models with latent variables. It consists of two main steps: the Expectation step (E-step) and the Maximization step (M-step).

In the context of clustering within a finite mixture model, the EM algorithm considers cluster assignments as latent variables. 
The EM algorithm for mixture models assumes that a set of latent variables $Z$ indicate which component of the mixture each observation originates from.

During the E-step, the algorithm estimates the expected value of the log-likelihood function, with respect to the conditional distribution of the latent variables given the observed data and the current estimates of the parameters. This step can be formally expressed as follows:
\begin{equation}
Q(\theta | \theta^{(t)}) = E_{Z|X,\theta^{(t)}}[\log L(\theta; X, Z)]
\end{equation}
where \( \theta \) denotes the parameter vector, \( X \) represents the observed data, \( Z \) are the latent variables, \( L \) is the likelihood function, and \( \theta^{(t)} \) are the parameter estimates from the previous iteration.

In the M-step, the algorithm maximizes the expected log-likelihood found in the E-step with respect to the parameters to obtain new parameter estimates:
\begin{equation}
\theta^{(t+1)} = \arg \max_{\theta} Q(\theta | \theta^{(t)})
\end{equation}

\subsubsection{E-step (Expectation Step)}

During the Expectation step, the EM algorithm computes the expected value of the log likelihood function, with respect to the conditional distribution of the latent variables given the observed data under the current estimate of the parameters. This step involves calculating the posterior probabilities that a given data point belongs to each of the $G$ components, based on the current estimates of the parameters.

For a mixture model, the posterior probability (also known as the responsibility) that observation i originates in component g is calculated as:

\begin{equation}
Z_{ig} = \frac{\pi_g f_g(\mathbf{y_i}|\theta_g)}{\sum_{j=1}^{G} \pi_j f_j(\mathbf{y_i} \mid \theta_j)}
\end{equation}

\subsubsection{M-step (Maximization Step)}

In the Maximization step, the EM algorithm updates the parameters of the model to maximize the expected log likelihood using the latest latent values $Z_{ig}$ from the E step. 
This involves updating the estimates of both the parameters of the component distributions and the mixing coefficients.

Update the mixing coefficients:

\begin{equation}
\pi_g^{new} = \frac{1}{N} \sum_{i=1}^{N} Z_{ig}
\end{equation}

The remaining parameters are updated using numerical optimization.

% 2. Update the parameters of the component distributions ($\theta_k$), which depends on the form of the distribution. For example, in a Gaussian mixture model, the mean and covariance of each Gaussian component are updated as follows:

% \begin{equation}
% \mu_k^{new} = \frac{\sum_{i=1}^{N} \gamma(z_{ik}) x_i}{\sum_{i=1}^{N} \gamma(z_{ik})}
% \end{equation}

% \begin{equation}
% \Sigma_k^{new} = \frac{\sum_{i=1}^{N} \gamma(z_{ik}) (x_i - \mu_k^{new})(x_i - \mu_k^{new})^T}{\sum_{i=1}^{N} \gamma(z_{ik})}
% \end{equation}

% where $N$ is the total number of data points.

% abc

% \section{Expectation-Maximization Algorithm for Clustering in Finite Mixture Models}


\section{Cluster Prediction for Ordinal Data}

Finite mixture models, combined with ordered stereotype regression, offer a technique for cluster prediction in ordinal data analysis. This method posits that a population can be probabilistically segmented into more homogeneous clusters, potentially enhancing prediction accuracy.

In our study, we will concentrate on row clusters with (Ordered Stereotype Model) OSM as an example to demonstrate our prediction method. It is feasible to apply this approach to other types, such as column clusters, or to models like POM.

\subsection{Data Structuring and Model Training}

The observed data ($Y$) is divided into a training set ($Y'$) and a test set ($Y''$). The training set comprises the first $t$ observations for adjusting model parameters, while the subsequent observations ($y_{t+1}, \dots, y_n$) make up the test set for evaluating the model.

The Expectation-Maximization (EM) algorithm is used to refine the parameters of the finite mixture model using the training data. Here, $K$ represents the number of ordinal categories, and $G$ denotes the number of clusters. The aggregate probability for each category, summed across all clusters, must equal one.

During the training phase, the M-step of the EM algorithm updates the parameters, $\mu_k$ and $\phi_k$ for each category, $\alpha_g$ and $\hat{\beta}_j$ for each cluster.

Upon completion of the EM algorithm, we obtain their estimates $\hat{\mu}_k$, $\hat{\phi}_k$, $\hat{\alpha}_g$ and $\hat{\beta}_j$ which are utilized for cluster prediction.

To achieve our objectives, the implementation will utilize the "clustord" package (\cite{clustord2024}), 
developed by the School of Mathematics and Statistics at Victoria University of Wellington. 
This package integrates the Ordered Stereotype Model (OSM) (\cite{fernandez2016mixture}), the Proportional Odds Model (POM) (\cite{matechou2016biclustering}), and several binary methods (\cite{pledger2014multivariate}), 
providing the ability to fit clustering models incorporating a wide array of covariates.

In our research, the parameter training stage for the OSM, as facilitated by the clustord package, is conducted following Fernandez's methodology (\cite{fernandez2016mixture}).

\subsection{Prediction and Validation}

After training, in the prediction stage the prediction will be applied in each row of the new data.

The posterior probability of row $i$ being in cluster g is:

% \begin{equation}
% PP_irg = \sum_{L}^{j=1} e^{\mu_k + \phi_k \cdot \alpha_g}
% \end{equation}

\begin{equation}
  \hat{Z}_{ig} = \frac{\hat{\pi}_g f_g(\mathbf{y_i}|\hat{\theta}_g)}{\sum_{j=1}^{G} \hat{\pi}_j f_j(\mathbf{y_i} \mid \hat{\theta}_j)}
\end{equation}

Then, the prediction of the cluster of row would be maximum $\hat{Z}_{ig}$ over all clusters g.

The model's accuracy is measured by how well the predicted clusters match the actual test data classifications.

\section{Experiments}

\subsection{Data Simulations}

In this section, we conduct experiments based on data generated from specified OSM parameters to analyze the effects of cluster and category distributions.

\subsubsection{OSM (Ordered Stereotype Model) Simulation}
We systematically vary individual model parameters while holding others constant to observe their influence on data density, particularly focusing on the parameter $\alpha$. The default parameters used in the simulations are as follows:
\[
\begin{aligned}
G &= 2, \\
q &= 3, \\
\alpha &= \mathbf{c}(1, -1), \\
\beta &= \mathbf{c}(0), \\
\mu &= \mathbf{c}(0, 0, 0), \\
\phi &= \mathbf{c}(0, 0.5, 1), \\
\pi &= \mathbf{c}(0.5, 0.5).
\end{aligned}
\]
All parameters except for $\beta$ are applied to a single outcome variable $Y$. This approach allows us to demonstrate the effects of adjusting a specific parameter on data density.

\subsubsection*{Effect of Different $\alpha$ Values}
In the OSM framework, $alpha$ is the argument of clusters. the sum of the $\alpha$ parameters is constrained to equal zero. 
We examine the impact of varying the $\alpha$ values using the following configurations:
\[
\begin{aligned}
\alpha_1 &= \mathbf{c}(-0.1, 0.1), \\
\alpha_2 &= \mathbf{c}(-1, 1), \\
\alpha_3 &= \mathbf{c}(-3, 3).
\end{aligned}
\]
We set cluster 1 has negitive $alpha$ value and cluster 2 has postive value.
As shown in Figure~\ref{fig:alpha}, 
the cluster has high $alpha$ value leading to have larger probability to have sample in high category.
And cluster has lower $alpha$ value have larger probability to have sample in low category.

\begin{figure}[ht]
  \centering
  \begin{subfigure}{1.0\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/para_sim/alpha.png}
  \end{subfigure}
  \caption{effect to cluster distribution from difference $\alpha$ value}
  \label{fig:alpha}
\end{figure}

% mu
\subsubsection*{Effect of Different $\mu$ Values}
In OSM, $mu$ is argument of category. 
The first value must be zero. There is no value limitation of $mu$, it can be large or negative.
To investigate the impact of varying $\mu$ values on the cluster distributions, 
we consider the following configurations:
\[
\begin{aligned}
\mu_1 &= \mathbf{c}(0, -1, -2), \\
\mu_2 &= \mathbf{c}(0, 2, 1), \\
\mu_3 &= \mathbf{c}(0, 1, 2).
\end{aligned}
\]
Figure~\ref{fig:mu} illustrates the effects of different $\mu$ value.
It shows the category has high $mu$ value leading to high probability of sample.
we also can see, $\mu_1$ and $\mu_3$ plots are showing left and right mirror. 
and the $\mu_2$ is in the middle.
( TODO: This is related to defalut $phi$ value. 
Need to check the detial reason.)

\begin{figure}[ht]
  \centering
  \begin{subfigure}{1.0\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/para_sim/mu.png}
  \end{subfigure}
  \caption{effect to cluster distribution from difference $\mu$ value}
  \label{fig:mu}
\end{figure}

% phi
\subsubsection*{Effect of Different $\phi$ Values}
The parameter $\phi$ represents the ordinal effect for each category, reflecting the cumulative probability across ordered categories. 
Importantly, the $\phi$ values must start from 0 and the $\phi$ value of the last category must be 1. 
To evaluate the impact of different $\phi$ values, we consider the following configurations:
\[
\phi_1 = \mathbf{c}(0, 0.2, 1), \quad \phi_2 = \mathbf{c}(0, 0.5, 1), \quad \phi_3 = \mathbf{c}(0, 0.8, 1).
\]
As shown in Figure~\ref{fig:phi}, 
when the probability associated with the second category increases (as $\phi$ values rise), 
the density distribution changes significantly between the clusters. 
Specifically, in Cluster 1, there is a slight decrease in the density of the second category, 
while Cluster 2 exhibits a clear increase in the density of the same category. 
This indicates that higher $\phi$ values lead to a greater disparity 
in the density distributions between the clusters, 
particularly affecting the category with the increased $\phi$ value.

Overall, these results suggest that larger $\phi$ values amplify the differences in density between categories across clusters, 
with a more pronounced effect on the cluster associated with the higher $\phi$ values.

\begin{figure}[ht]
  \centering
  \begin{subfigure}{1.0\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/para_sim/phi.png}
  \end{subfigure}
  \caption{effect to cluster distribution from difference $\phi$ value}
  \label{fig:phi}
\end{figure}

% beta
\subsubsection*{different $\beta$ value}
The parameter $\beta$ represents the effect of each column in the data. 
Figure~\ref{fig:beta} illustrates three different sets of $\beta$ values across five columns. 
When the $\beta$ values are close to zero, the impact on the data distribution is minimal. 
In Figure~\ref{fig:beta} a, the sampling distributions are nearly identical to the scenario where $\beta = 0$, 
indicating no significant effect from the columns. 
As we compare Figures~\ref{fig:beta} a, b, and c, it becomes evident that the effects 
from the columns increase as the $\beta$ values deviate further from zero.

Additionally, negative $\beta$ values result in a higher probability of sampling from the lower categories, 
whereas positive $\beta$ values increase the likelihood of sampling from the higher categories.
\begin{figure}[ht]
  \centering
  \begin{subfigure}{1.0\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/para_sim/beta_1.png}
      \caption*{a. samll difference between $\beta$}
  \end{subfigure}

  \begin{subfigure}{1.0\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/para_sim/beta_2.png}
      \caption*{a. medim difference between $\beta$}
    \end{subfigure}

  \begin{subfigure}{1.0\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/para_sim/beta_3.png}
      \caption*{a. large difference between $\beta$}
  \end{subfigure}
  
  \caption{Effect to cluster distribution from different $\beta$ values}
  \label{fig:beta}
\end{figure}

% pi
\subsubsection*{Effect of Different $\pi$ Values}
In EM Algorithm, the $\pi$ parameter effects the sample size of each cluster. 
In Figure~\ref{fig:pi} shows when $\pi$ value small leading to less sample size.
when $\pi$ larger then the cluster has more sample size.
When $\pi$ same then the overall sample size is close.
\begin{figure}[ht]
  \centering
  \begin{subfigure}{1.0\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/para_sim/pi.png}
  \end{subfigure}
  \caption{effect to cluster distribution from difference $\pi$ value}
  \label{fig:pi}
\end{figure}

% Normal Dist Simu Section
\subsubsection{Normal Distributions Simulation}

In order to better simulate the actual situation.
In this work, we introduced another approach to simulation the ordinal data 
for our prediction experiments. 
This approach is based on assume the data in each cluster are following 
a known distribution with known parameter of that distribution.
And the value of each categories are selected by cuts which same to each clusters.

In Cluster distributions, our work is based on two clusters and three categories.

There are 4 difference cluster distributions settings, which shown in 
Figure~\ref*{fig:dist_sim} (a) clusters follow same distribution $N(0, 1)$ 
with same two cuts at $-1$ and $1$,
Figure~\ref*{fig:dist_sim} (b) clusters follow different distribution $N(-1,1)$ and $N(-1.5,2)$
with same two cuts at $-1$ and $1$,

Figure~\ref*{fig:dist_sim} (c) clusters follow different distribution with closer means $N(0,6)$ and $N(3,8)$
with same two cuts at $0$ and $2.5$,

Figure~\ref*{fig:dist_sim} (d) clusters follow different distribution with far means $N(0,1)$ and $N(4,2)$
with same two cuts at $1$ and $2.5$,

\begin{figure}[ht]
  \centering
  \begin{subfigure}{0.32\textwidth}  % Adjust width to 0.32 to fit three images in a row
    \centering
    \includegraphics[width=\textwidth]{images/dist_simu/nor_close.png} % Replace with your second image path
    \caption{Clusters follow different distribution with close center\\ cut1=0, cut2=2.5}
\end{subfigure}
  \hfill
  \begin{subfigure}{0.32\textwidth}  % Adjust width to 0.32 to fit three images in a row
    \centering
    \includegraphics[width=\textwidth]{images/dist_simu/nor.png} % Replace with your first image path
    \caption{Clusters follow different distribution with same cuts\\ cut1=-1, cut2=1}
\end{subfigure}
  \hfill
  \begin{subfigure}{0.32\textwidth}  % Adjust width to 0.32 to fit three images in a row
      \centering
      \includegraphics[width=\textwidth]{images/dist_simu/nor_far.png} % Replace with your third image path
      \caption{Clusters follow different distribution with far center\\ cut1=1, cut2=2.5}
  \end{subfigure}
  
  \caption{Normal Distribution Simulations: with 3 clusters. the sample value if less then cut 1 is category 1, between cut 1 and cut 2 is category 2, and larger than cut 2 is category 3.}
  \label{fig:dist_sim}
\end{figure}


\subsection{Experiments approach}

In this study, we divided the simulated data into training and validation sets.

First, the Expectation-Maximization (EM) algorithm was applied to estimate the parameter values from the training data, using only the response variable $Y$.

Next, the procedure outlined in Algorithm~\ref{fig:algo} was employed to calculate the probability of each category, cluster, and column combination. 
These probabilities were then standardized within each cluster.

Finally, for each new observation $Y'$ in the validation set, we computed the Z-score for each cluster using these standardized probabilities.
\begin{algorithm}
  \caption{Pseudocode for Calculating Cluster Probabilities}
  \label{fig:algo}
  \begin{algorithmic}[1]
  \STATE Initialize a 3D array \texttt{cluster\_probs} with dimensions $G \times \texttt{number\_of\_y} \times q$ and all elements set to 0.
  
  \FOR{$g = 1$ to $G$}
      \FOR{$j = 1$ to \texttt{number\_of\_y}}
          \FOR{$k = 1$ to $q$}
              \IF{$k > 1$}
                  \STATE Calculate $\texttt{linear} \gets \mu[k] + \phi[k] \times (\alpha[g] + \beta[j])$
                  \STATE Set $\texttt{cluster\_probs}[g, j, k] \gets \exp(\texttt{linear})$
              \ELSE
                  \STATE Set $\texttt{cluster\_probs}[g, j, k] \gets 1$
              \ENDIF
          \ENDFOR
          \STATE Normalize $\texttt{cluster\_probs}[g, j,]$ by dividing each element by the sum of all elements in that row.
      \ENDFOR
  \ENDFOR
  
  \RETURN \texttt{cluster\_probs}
  \end{algorithmic}
  \end{algorithm}


\section{Experiment Results}

\subsection{Prediction with OSM simulate}

\subsubsection*{Prediction with differet number of Ys}

This experiments use following parameter value.
\[
\begin{aligned}
G &= 2, \\
q &= 3, \\
\alpha &= \mathbf{c}(1, -1), \\
\mu &= \mathbf{c}(0, 0, 0), \\
\phi &= \mathbf{c}(0, 0.5, 1), \\
\pi &= \mathbf{c}(0.5, 0.5).
\end{aligned}
\]
\[
\beta \sim \mathcal{U}(-1, 1)
\]
For 20 Ys Experiment, we generated 20 values of $\beta$ from a uniform distribution over the interval $[-1, 1]$.
And in 30 Ys, 50 Ys experiments, $\beta$ generated from 30 and 50 values from uniform distribution as well.
In ~\ref*{tab:20_ys}, ~\ref*{tab:30_ys} and ~\ref*{tab:50_ys} shows the confusion matrix of these prediction result of different number of Ys.

\subsubsection*{Prediction with differet number of Cluster}

In both experiments of different number of cluster, 
the default parameter values were used across different clusters, 
with 20 values of Y generated for each experiment. 
The EM algorithm was configured with the following parameters: EM cycles = 100, start EM cycles = 5, and nstarts = 5.

For the three-cluster experiment, the values of $\alpha$ were set to -1, 0, and 1, 
while the cluster proportions ($\pi$) were 0.33, 0.34, and 0.33, respectively. 

In the five-cluster experiment, $\alpha$ values were set to -1, -0.5, 0, 0.5, and 1, 
with equal cluster proportions of 0.2 for each cluster.

In ~\ref*{tab:2_clu}, ~\ref*{tab:3_clu} and ~\ref*{tab:5_clu} shows the confusion matrix of these prediction result of different number of clusters.

From ~\ref*{fig:exp_res} we found the prediciton overall accuracy of cluster.
It keep increase when number of Ys increased,
But it shows significant fast decreasing when number of cluster is increased.

\begin{figure}[ht]
  \centering
  \begin{subfigure}{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/experiments/dif_ys.png}
      \caption{Prediction Accuracy with different number of Ys}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{images/experiments/exp_diff_clusters.png} % Replace with your second image path
      \caption{Prediction Accuracy with different number of Clusters}
  \end{subfigure}
  \caption{Cluster Prediction Experiments Results}
  \label{fig:exp_res}
\end{figure}

\begin{table}[ht]
  \centering

  % Title above all tables
  \caption*{\textbf{Confusion matrix of different number of Ys}}

  % First row of tables (2 side by side)
  \begin{minipage}{0.45\textwidth}
    \centering
    \begin{tabular}{c|c|c|c}
              & \textbf{Reference} & 1 & 2 \\
    \hline
    \textbf{Prediction} & 1 & 283 & 0 \\
                        & 2 & 15 & 293 \\
    \end{tabular}
    \caption{20 Ys}
    \label{tab:20_ys}
  \end{minipage}
  \hfill
  \begin{minipage}{0.45\textwidth}
    \centering
    \begin{tabular}{c|c|c|c}
              & \textbf{Reference} & 1 & 2 \\
    \hline
    \textbf{Prediction} & 1 & 289 & 4 \\
                        & 2 & 9 & 298 \\
    \end{tabular}
    \caption{30 Ys}
    \label{tab:30_ys}
  \end{minipage}
  
  \vspace{1em} % Space between rows
  
  % Second row (1 table centered)
  \begin{minipage}{0.6\textwidth}
    \centering
    \begin{tabular}{c|c|c|c}
              & \textbf{Reference} & 1 & 2 \\
    \hline
    \textbf{Prediction} & 1 & 297 & 10 \\
                        & 2 & 1 & 302 \\
    \end{tabular}
    \caption{50 Ys}
    \label{tab:50_ys}
  \end{minipage}

\end{table}

% confustion matrix of diff clusters

\begin{table}[ht]
  \centering

  % Title above all tables
  \caption*{\textbf{Confusion matrix of different number of Clusters}}

  % First row of tables (2 side by side)
  \begin{minipage}{0.45\textwidth}
    \centering
    \begin{tabular}{c|c|c|c}
              & \textbf{Reference} & 1 & 2 \\
    \hline
    \textbf{Prediction} & 1 & 283 & 0 \\
                        & 2 & 15 & 293 \\
    \end{tabular}
    \caption{2 Clusters}
    \label{tab:2_clu}
  \end{minipage}
  \hfill
  \begin{minipage}{0.45\textwidth}
    \centering
    \begin{tabular}{c|c|c|c|c}
      & \textbf{Reference} & \textbf{1} & \textbf{2} & \textbf{3} \\
      \hline
      \textbf{Prediction} & \textbf{1} & 334 & 63 & 0 \\
                          & \textbf{2} & 110 & 383 & 152 \\
                          & \textbf{3} & 1 & 39 & 268 \\
    \end{tabular}
    \caption{3 Clusters}
    \label{tab:3_clu}
  \end{minipage}
  
  \vspace{1em} % Space between rows
  
  % Second row (1 table centered)
  \begin{minipage}{0.6\textwidth}
    \centering
    \begin{tabular}{c|c|c|c|c|c|c}
      & \textbf{Reference} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} \\
      \hline
      \textbf{Prediction} & \textbf{1} & 0 & 0 & 0 & 0 & 0 \\
                          & \textbf{2} & 505 & 201 & 69 & 6 & 0 \\
                          & \textbf{3} & 0 & 0 & 0 & 0 & 0 \\
                          & \textbf{4} & 243 & 543 & 730 & 722 & 731 \\
                          & \textbf{5} & 0 & 0 & 0 & 0 & 0 \\
    \end{tabular}
    \caption{5 Clusters}
    \label{tab:5_clu}
  \end{minipage}

\end{table}


\subsection{Prediction with Normal Distribution simulate}

The experiments, based on different Normal distribution simulations, 
utilized the same EM algorithm and were conducted exclusively for two clusters, 
accounting for column effects as well.

As shown in ~\ref*{fig:dist_acc} , all experiments resulted in very high overall accuracy. 
Additionally, the confusion matrices in Tables ~\ref*{tab:nor_close}, ~\ref*{tab:nor_far} and  ~\ref*{tab:nor_cuts}
demonstrate that the predictions for each cluster are highly accurate.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\textwidth]{images/experiments/norm_dist.png}
  \caption{Prediction Accuracy for Normal with different parameter and cuts}
  \label{fig:dist_acc}
\end{figure}


\begin{table}[ht]
  \centering

  % Title above all tables
  \caption*{\textbf{Confusion matrix of Normal Distributions}}

  \begin{minipage}{0.45\textwidth}
    \centering
    \begin{tabular}{c|c|c|c}
              & \textbf{Reference} & 1 & 2 \\
    \hline
    \textbf{Prediction} & 1 & 147 & 0 \\
                        & 2 & 0 & 153 \\
    \end{tabular}
    \caption{Far center}
    \label{tab:nor_far}
  \end{minipage}
  \hfill
  \begin{minipage}{0.45\textwidth}
    \centering
    \begin{tabular}{c|c|c|c}
              & \textbf{Reference} & 1 & 2 \\
    \hline
    \textbf{Prediction} & 1 & 144 & 0 \\
                        & 2 & 3 & 153 \\
    \end{tabular}
    \caption{Close center}
    \label{tab:nor_close}
  \end{minipage}

  \vspace{1em} % Space between rows

  \begin{minipage}{0.45\textwidth}
    \centering
    \begin{tabular}{c|c|c|c}
              & \textbf{Reference} & 1 & 2 \\
    \hline
    \textbf{Prediction} & 1 & 147 & 0 \\
                        & 2 & 0 & 153 \\
    \end{tabular}
    \caption{different distributions with same cuts}
    \label{tab:nor_cuts}
  \end{minipage}
\end{table}

\subsection{Other topic may need to explained}

\subsubsection{Row cluster}

TODO

\subsubsection{Row cluster with column effects}

TODO


\subsection{Labeling switch}

TODO

\section{Conclusion and Future Directions}

TODO

\printbibliography

\end{document}
